{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2b2eba-b7fd-4856-960f-f2cbadcc12af",
   "metadata": {},
   "source": [
    "# Building a Metaphor Data Agent\n",
    "\n",
    "This tutorial walks through using the LLM tools provided by the [Metaphor API](https://platform.metaphor.systems/) to allow LLMs to easily search and retrieve HTML content from the Internet.\n",
    "\n",
    "To get started, you will need an [OpenAI api key](https://platform.openai.com/account/api-keys) and a [Metaphor API key](https://dashboard.metaphor.systems/overview).\n",
    "\n",
    "We will import the relevant agents and tools and pass them our keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2a0ecd-22e9-4cef-b069-89e4286e4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "retrieve_documents\n",
      "current_date\n",
      "find_similar\n"
     ]
    }
   ],
   "source": [
    "# Set up OpenAI\n",
    "import openai\n",
    "from llama_index.agent import OpenAIAgent\n",
    "openai.api_key = 'sk-your-key'\n",
    "\n",
    "# Set up Metaphor tool\n",
    "from llama_hub.tools.metaphor.base import MetaphorToolSpec\n",
    "metaphor_tool = MetaphorToolSpec(\n",
    "    api_key='your-key',\n",
    ")\n",
    "\n",
    "metaphor_tool_list = metaphor_tool.to_tool_list()\n",
    "for tool in metaphor_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e3012-bab0-4e55-858a-e3721282552c",
   "metadata": {},
   "source": [
    "## Testing the Metaphor tools\n",
    "\n",
    "We've imported our OpenAI agent, set up the api key, and initalized our tool, checking the methods that it has available. Let's test out the tool before setting up our Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64da618-b4ab-42d7-903d-f4eeb624f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Illustrated Guide to Transformers- Step by Step Explanation',\n",
       "  'url': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?gi=8fe76db5c4d9',\n",
       "  'id': 'czTCPZ1vqo-f92WQwKvRig'},\n",
       " {'title': 'Transformers, explained: Understand the model behind GPT, BERT, and T5',\n",
       "  'url': 'https://www.youtube.com/watch?v=SZorAJ4I-sA&feature=youtu.be',\n",
       "  'id': 'VkJuLxzjSgX8-OjN2Huwqg'},\n",
       " {'title': 'A Deep Dive Into the Transformer Architecture ‚Äî The Development of Transformer Models',\n",
       "  'url': 'https://towardsdatascience.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models-acbdf7ca34e0?gi=f20b470a0326',\n",
       "  'id': 'UssKoV3Wje2RyicVGcbwuQ'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.search('This is the best explanation for machine learning transformers:', num_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54eff99-7a6f-4e97-a037-5973bac502f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d22776c9-bf0d-4642-911c-55e03a0127fd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bbd2ad7f78cf02100add6077decdf885ffebfe37743bfc13e80e61b5b833e46c', text='<div><div><p>\\n <a href=\"/static/transformers-bumblebee-d4ec62a6a46aae8f83b7419242f59f4e-48854.jpg\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>In this post, we will be describing a class of sequence processing models known as Transformers (‚Ä¶<em>robots in disguise</em>).\\nJokes aside, Transformers came out on the scene not too long ago and have rocked the natural language processing community because of\\ntheir pitch: state-of-the-art and efficient sequence processing <strong>without recurrent units or convolution</strong>. </p>\\n<p><em>‚ÄúNo recurrent units or convolution?! What are these models even made of?!‚Äù</em>, you may be exclaiming to unsuspecting strangers on the streets. </p>\\n<p>Not much it turns out, other than a bunch of attention and feedforward operations. </p>\\n<p>While the individual components that make up the Transformer model are not particularly novel, this is still a pretty dense paper with a lot\\nof moving parts. So our aim in this post will be to distill the model to its key contributions, without getting too stuck\\nin the details.</p>\\n<p>But first, the TLDR for the paper:</p>\\n<ol>\\n<li><strong>Transformers demonstrate that recurrence and convolution are not essential for building high-performance natural language models</strong></li>\\n<li><strong>They achieve state-of-the-art machine translation results using a self-attention operation</strong></li>\\n<li><strong>Attention is a highly-efficient operation due to its parallelizability and runtime characteristics</strong></li>\\n</ol>\\n<p>If that sounds exciting, read onward!</p>\\n<h2>How Transformers Work</h2>\\n<p>While the Transformer does not use traditional recurrent units or convolutions, it still takes inspiration from\\nsequence-to-sequence architectures where we encode some input and iteratively decode a desired output. </p>\\n<p>How does this play out in practice? Let‚Äôs focus on the encoder first. There are quite a few elements to the process,\\nso don‚Äôt get too lost in the details. All we are doing is encoding some inputs üôÇ.</p>\\n<p>Assume we start with a certain phrase that we would like to translate from Spanish to English. The Transformer\\nbegins by embedding the tokens of the Spanish phrase into a conventional embedding matrix:</p>\\n<p>\\n <a href=\"/static/input_embedding_matrix-a2e9df88621a1dae9c4c16b097b808ae-1c473.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>Because the model makes no use of recurrence, we need some way to represent position-based information\\nin the model. Hence we add a positional encoding to this embedding matrix, whose exact form we will describe\\nin the next section:</p>\\n<p>\\n <a href=\"/static/positional_encoding_added-9eb9de50b7ddfe4287ef9e386649e292-d8e49.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>Our modified input is fed into the first layer of the Transformer encoder. Within each encoder layer,\\nwe perform a series of operations on the inputs.</p>\\n<p>First off, we feed the input through a multi-head attention operation:</p>\\n<p>\\n <a href=\"/static/multihead_attention_encoder-503fd3d322872609873e62cbb1ca98d4-71c2a.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>To this attention output, we also add a residual connection as well as perform a layer normalization step:</p>\\n<p>\\n <a href=\"/static/residual_layer_to_multihead_attention-8249f2237caed6af900dabf68ef86f39-14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.retrieve_documents(['iEYMai5rS9k0hN5_BH0VZg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55a0c7b-4c58-4725-8543-29bb1b7278ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Transformers: Attention in Disguise',\n",
       "  'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n",
       "  'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n",
       " {'title': 'Transformers: a Primer',\n",
       "  'url': 'http://www.columbia.edu/~jsl2239/transformers.html',\n",
       "  'id': 'cXbT9IsR5u8NtLTAIcUWOA'},\n",
       " {'title': 'Illustrated Guide to Transformers- Step by Step Explanation',\n",
       "  'url': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?gi=8fe76db5c4d9',\n",
       "  'id': 'czTCPZ1vqo-f92WQwKvRig'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.find_similar('https://www.mihaileric.com/posts/transformers-attention-in-disguise/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c7b98-0d75-4ef0-ac47-fd3bd24d3e50",
   "metadata": {},
   "source": [
    "## Avoiding Context Window Problems\n",
    "\n",
    "The above example shows the main uses of the Metaphor tool. We can easily retrieve a clean list of links related to a query, and then we can fetch the content of the article as a cleaned up html extract. \n",
    "\n",
    "We can see that the content of the articles is somewhat long compared to current LLM context windows, and so to allow retrieval and summary of many documents we will set up and use another tool from LlamaIndex that allows us to load text into a VectorStore, and query it for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a017cc61-1696-4a03-8d09-a628f9049cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# The retrieve documents tool is the second in the tool list, as seen above\n",
    "wrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n",
    "    metaphor_tool_list[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b47437-8f6d-4e94-97ca-4e35f78336f2",
   "metadata": {},
   "source": [
    "Our wrapped retrieval tools seperate loading and reading into seperate interfaces. We use `load` to load the documents into the vector store, and `read` to query the vector store. Let's try it out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f81bd3-a5b9-452c-93f4-91d16c4c0df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA Transformer is a class of sequence processing models that use attention and feedforward operations to achieve state-of-the-art and efficient sequence processing without recurrent units or convolution.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_retrieve.load(['iEYMai5rS9k0hN5_BH0VZg'])\n",
    "wrapped_retrieve.read('what is a transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be6977-c4e8-43a4-99be-3322d4b72b07",
   "metadata": {},
   "source": [
    "## Creating the Agent\n",
    "\n",
    "We now have a total of 5 tools that we will make available to the Agent, allowing it to use Metaphor's services to it's full potential:\n",
    "\n",
    "`search`: To search for a list of relevant articles\n",
    "\n",
    "`retrieve_documents`: To load the articles into a vector store\n",
    "\n",
    "`read_retrieve_documents`: To query the articles loaded into the vector store\n",
    "\n",
    "`find_similar`: To find articles similiar to a given URL\n",
    "\n",
    "`get_date`: A simple utility method to get todays date for the Agent, for better usage of date based filtering\n",
    "\n",
    "\n",
    "We will pass all of these functions into our Agent, and test it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a893f26-dbb6-4b72-9795-702eaf749564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [metaphor_tool_list[0], *wrapped_retrieve.to_tool_list(), metaphor_tool_list[2], metaphor_tool_list[3]],\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5835d058-da9c-4d42-9d2a-941c73b88a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: current_date with args: {}\n",
      "Got output: 2023-08-16\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: search with args: {\n",
      "  \"query\": \"news about superconductors\",\n",
      "  \"start_published_date\": \"2023-07-16\",\n",
      "  \"end_published_date\": \"2023-08-16\"\n",
      "}\n",
      "Got output: [{'title': 'Get ready for the age of the superconductor (maybe)', 'url': 'https://newatlas.com/science/get-ready-for-the-age-of-the-superconductor-maybe/', 'id': '1-PPJlmzmhdzV8cjvGftLw'}, {'title': 'After LK-99 letdown, scientists still hopeful to find ‚ÄòHoly Grail‚Äô superconductor - WTOP News', 'url': 'https://wtop.com/tech/2023/08/after-lk-99-letdown-scientists-still-hopeful-to-find-holy-grail-superconductor/', 'id': '93B_9_V-66be9IAhiejhAQ'}, {'title': 'Team creates simple superconducting device that could dramatically cut energy use in computing', 'url': 'https://phys.org/news/2023-07-team-simple-superconducting-device-energy.html', 'id': 'rdfzvO08RiLEF1O-ihfOmQ'}, {'title': 'Korean team claims to have created the first room-temperature, ambient-pressure superconductor', 'url': 'https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html', 'id': 'jXsm1TpKVy9zg5kH3bRuag'}, {'title': 'Where Will Practical Room Temperature Superconductors Matter? | NextBigFuture.com', 'url': 'https://www.nextbigfuture.com/2023/07/where-will-practical-room-temperature-superconductors-matter.html', 'id': 'tFMhm6UNpyuUUeHMeEyyVg'}, {'title': 'Why a \"room-temperature superconductor\" would be a huge deal', 'url': 'https://www.vox.com/future-perfect/23816753/superconductor-room-temperature-lk99-quantum-fusion', 'id': 'cD13waRIIwL6_S4SlIRHjQ'}, {'title': 'Forget AI. For a moment Silicon Valley was obsessed with floating rocks.', 'url': 'https://www.washingtonpost.com/technology/2023/08/11/superconductors-hype-lk99-silicon-valley/', 'id': 'k8Sf1BuW_FK_W1KQvBT08w'}, {'title': 'Analysis | What‚Äôs an Ambient Superconductor and Why the Buzz About LK-99?', 'url': 'https://www.washingtonpost.com/business/energy/2023/08/03/room-temperature-superconductor-and-lk-99-what-to-know/ec47ab44-31ce-11ee-85dd-5c3c97d6acda_story.html', 'id': 'f3ufczqrMRFp-rlFsE14cQ'}, {'title': \"Room-temperature superconductor 'breakthrough' met with scepticism\", 'url': 'https://www.newscientist.com/article/2384782-room-temperature-superconductor-breakthrough-met-with-scepticism/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home', 'id': 'sl8QkKlqm1IWASBsfDcg2A'}, {'title': 'Scientists claim room-temp superconductor breakthrough', 'url': 'https://www.theregister.com/2023/07/27/room_temperature_superconductor_paper/', 'id': 'xHWjiZoTjqMcgSaCKS3Hsg'}]\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: retrieve_documents with args: {\n",
      "  \"ids\": [\"1-PPJlmzmhdzV8cjvGftLw\", \"93B_9_V-66be9IAhiejhAQ\", \"rdfzvO08RiLEF1O-ihfOmQ\", \"jXsm1TpKVy9zg5kH3bRuag\", \"tFMhm6UNpyuUUeHMeEyyVg\", \"cD13waRIIwL6_S4SlIRHjQ\", \"k8Sf1BuW_FK_W1KQvBT08w\", \"f3ufczqrMRFp-rlFsE14cQ\", \"sl8QkKlqm1IWASBsfDcg2A\", \"xHWjiZoTjqMcgSaCKS3Hsg\"]\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_retrieve_documents\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: read_retrieve_documents with args: {\n",
      "  \"query\": \"summarize all\"\n",
      "}\n",
      "Got output: \n",
      "Researchers at the University of Rochester have developed a new type of superconductive integrated circuit system that uses five times less power and operates at least 10-100 times faster than traditional systems. This new technology could help reduce the amount of energy needed to support cloud computing, which currently accounts for five percent of the total energy on the planet. The effect is due to a simple, straightforward property of superconductors that was discovered by the researchers. The work has been praised by the Max Planck Institute for the Structure and Dynamics of Matter for its potential to revolutionize the field.\n",
      "========================\n",
      "In the last month, several news articles have been published on superconductors. Here are some key highlights:\n",
      "\n",
      "1. \"Get ready for the age of the superconductor (maybe)\" - This article discusses the potential advancements in superconductors and their impact on various industries. [Read more](https://newatlas.com/science/get-ready-for-the-age-of-the-superconductor-maybe/)\n",
      "\n",
      "2. \"After LK-99 letdown, scientists still hopeful to find ‚ÄòHoly Grail‚Äô superconductor\" - Despite recent setbacks, scientists remain optimistic about discovering a groundbreaking superconductor. [Read more](https://wtop.com/tech/2023/08/after-lk-99-letdown-scientists-still-hopeful-to-find-holy-grail-superconductor/)\n",
      "\n",
      "3. \"Team creates simple superconducting device that could dramatically cut energy use in computing\" - Researchers have developed a new superconducting device that consumes less power and operates faster, potentially reducing energy consumption in computing. [Read more](https://phys.org/news/2023-07-team-simple-superconducting-device-energy.html)\n",
      "\n",
      "4. \"Korean team claims to have created the first room-temperature, ambient-pressure superconductor\" - A Korean team claims to have achieved a major breakthrough by creating the first room-temperature superconductor. [Read more](https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html)\n",
      "\n",
      "5. \"Where Will Practical Room Temperature Superconductors Matter?\" - This article explores the potential applications and significance of practical room temperature superconductors. [Read more](https://www.nextbigfuture.com/2023/07/where-will-practical-room-temperature-superconductors-matter.html)\n",
      "\n",
      "6. \"Why a 'room-temperature superconductor' would be a huge deal\" - This article discusses the significance of achieving room-temperature superconductivity and its potential impact on various fields. [Read more](https://www.vox.com/future-perfect/23816753/superconductor-room-temperature-lk99-quantum-fusion)\n",
      "\n",
      "These articles provide insights into the latest developments and advancements in the field of superconductors.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat('Can you summarize all of the news published in the last month on superconductors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee91ca-6730-4fdd-8189-ac21022f34f1",
   "metadata": {},
   "source": [
    "We asked the agent to retrieve documents related to superconductors from this month. It used the `get_date` tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling `search`. It then loaded the documents using `retrieve_documents` and read them using `read_retrieve_documents`.\n",
    "\n",
    "We can make another query to the vector store to read from it again, now that the articles are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d279f9f-c305-43ed-ae54-334c0ec6f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: read_retrieve_documents with args: {\n",
      "  \"query\": \"process to make new superconductor\"\n",
      "}\n",
      "Got output: \n",
      "The process to make the new superconductor involves substituting a fraction of the lead in the material with copper ions, resulting in tiny structural distortions. This deformation leads to the creation of superconducting quantum wells, which are key to achieving superconductivity. The material is then unable to relax and lose its superconductivity. The process may also involve using a laser, a diamond anvil, and a lot of pressure.\n",
      "========================\n",
      "The process to make the new superconductor involves substituting a fraction of the lead in the material with copper ions, which leads to tiny structural distortions. These distortions create superconducting quantum wells, which are essential for achieving superconductivity. The material is intentionally prevented from relaxing and losing its superconductivity. Additionally, the process may involve the use of a laser, a diamond anvil, and high pressure.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat('whats the process to make the new superconductor?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
