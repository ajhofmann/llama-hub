{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2b2eba-b7fd-4856-960f-f2cbadcc12af",
   "metadata": {},
   "source": [
    "# Building a Metaphor Data Agent\n",
    "\n",
    "This tutorial walks through using the LLM tools provided by the [Metaphor API](https://platform.metaphor.systems/) to allow LLMs to easily search and retrieve HTML content from the Internet.\n",
    "\n",
    "To get started, you will need an [OpenAI api key](https://platform.openai.com/account/api-keys) and a [Metaphor API key](https://dashboard.metaphor.systems/overview).\n",
    "\n",
    "We will import the relevant agents and tools and pass them our keys here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2a0ecd-22e9-4cef-b069-89e4286e4d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metaphor_search\n",
      "retrieve_documents\n",
      "current_date\n",
      "find_similar\n"
     ]
    }
   ],
   "source": [
    "# Set up OpenAI\n",
    "import openai\n",
    "from llama_index.agent import OpenAIAgent\n",
    "openai.api_key = 'sk-your-key'\n",
    "\n",
    "# Set up Metaphor tool\n",
    "from llama_hub.tools.metaphor.base import MetaphorToolSpec\n",
    "metaphor_tool = MetaphorToolSpec(\n",
    "    api_key='your-key',\n",
    ")\n",
    "\n",
    "metaphor_tool_list = metaphor_tool.to_tool_list()\n",
    "for tool in metaphor_tool_list:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e3012-bab0-4e55-858a-e3721282552c",
   "metadata": {},
   "source": [
    "## Testing the Metaphor tools\n",
    "\n",
    "We've imported our OpenAI agent, set up the api key, and initalized our tool, checking the methods that it has available. Let's test out the tool before setting up our Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64da618-b4ab-42d7-903d-f4eeb624f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Transformers: Attention in Disguise',\n",
       "  'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n",
       "  'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n",
       " {'title': 'An Analogy for Understanding Transformers - LessWrong',\n",
       "  'url': 'https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers',\n",
       "  'id': 'zKjHcPuGa9wuBT_EVDLrJg'},\n",
       " {'title': 'A Conceptual Guide to Transformers: Part I',\n",
       "  'url': 'https://benlevinstein.substack.com/p/a-conceptual-guide-to-transformers',\n",
       "  'id': 'hyRl0cnGaBk6RgIMpWMMew'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.metaphor_search('This is the best explanation for machine learning transformers:', num_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54eff99-7a6f-4e97-a037-5973bac502f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='551c0613-124b-45d8-985f-02b7c7a9e634', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bbd2ad7f78cf02100add6077decdf885ffebfe37743bfc13e80e61b5b833e46c', text='<div><div><p>\\n <a href=\"/static/transformers-bumblebee-d4ec62a6a46aae8f83b7419242f59f4e-48854.jpg\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>In this post, we will be describing a class of sequence processing models known as Transformers (‚Ä¶<em>robots in disguise</em>).\\nJokes aside, Transformers came out on the scene not too long ago and have rocked the natural language processing community because of\\ntheir pitch: state-of-the-art and efficient sequence processing <strong>without recurrent units or convolution</strong>. </p>\\n<p><em>‚ÄúNo recurrent units or convolution?! What are these models even made of?!‚Äù</em>, you may be exclaiming to unsuspecting strangers on the streets. </p>\\n<p>Not much it turns out, other than a bunch of attention and feedforward operations. </p>\\n<p>While the individual components that make up the Transformer model are not particularly novel, this is still a pretty dense paper with a lot\\nof moving parts. So our aim in this post will be to distill the model to its key contributions, without getting too stuck\\nin the details.</p>\\n<p>But first, the TLDR for the paper:</p>\\n<ol>\\n<li><strong>Transformers demonstrate that recurrence and convolution are not essential for building high-performance natural language models</strong></li>\\n<li><strong>They achieve state-of-the-art machine translation results using a self-attention operation</strong></li>\\n<li><strong>Attention is a highly-efficient operation due to its parallelizability and runtime characteristics</strong></li>\\n</ol>\\n<p>If that sounds exciting, read onward!</p>\\n<h2>How Transformers Work</h2>\\n<p>While the Transformer does not use traditional recurrent units or convolutions, it still takes inspiration from\\nsequence-to-sequence architectures where we encode some input and iteratively decode a desired output. </p>\\n<p>How does this play out in practice? Let‚Äôs focus on the encoder first. There are quite a few elements to the process,\\nso don‚Äôt get too lost in the details. All we are doing is encoding some inputs üôÇ.</p>\\n<p>Assume we start with a certain phrase that we would like to translate from Spanish to English. The Transformer\\nbegins by embedding the tokens of the Spanish phrase into a conventional embedding matrix:</p>\\n<p>\\n <a href=\"/static/input_embedding_matrix-a2e9df88621a1dae9c4c16b097b808ae-1c473.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>Because the model makes no use of recurrence, we need some way to represent position-based information\\nin the model. Hence we add a positional encoding to this embedding matrix, whose exact form we will describe\\nin the next section:</p>\\n<p>\\n <a href=\"/static/positional_encoding_added-9eb9de50b7ddfe4287ef9e386649e292-d8e49.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>Our modified input is fed into the first layer of the Transformer encoder. Within each encoder layer,\\nwe perform a series of operations on the inputs.</p>\\n<p>First off, we feed the input through a multi-head attention operation:</p>\\n<p>\\n <a href=\"/static/multihead_attention_encoder-503fd3d322872609873e62cbb1ca98d4-71c2a.png\">\\n \\n \\n \\n \\n \\n \\n \\n </a>\\n </p>\\n<p>To this attention output, we also add a residual connection as well as perform a layer normalization step:</p>\\n<p>\\n <a href=\"/static/residual_layer_to_multihead_attention-8249f2237caed6af900dabf68ef86f39-14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.retrieve_documents(['iEYMai5rS9k0hN5_BH0VZg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55a0c7b-4c58-4725-8543-29bb1b7278ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Transformers: Attention in Disguise',\n",
       "  'url': 'https://www.mihaileric.com/posts/transformers-attention-in-disguise/',\n",
       "  'id': 'iEYMai5rS9k0hN5_BH0VZg'},\n",
       " {'title': 'Transformers: a Primer',\n",
       "  'url': 'http://www.columbia.edu/~jsl2239/transformers.html',\n",
       "  'id': 'cXbT9IsR5u8NtLTAIcUWOA'},\n",
       " {'title': 'Illustrated Guide to Transformers- Step by Step Explanation',\n",
       "  'url': 'https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0?gi=8fe76db5c4d9',\n",
       "  'id': 'czTCPZ1vqo-f92WQwKvRig'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaphor_tool.find_similar('https://www.mihaileric.com/posts/transformers-attention-in-disguise/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c7b98-0d75-4ef0-ac47-fd3bd24d3e50",
   "metadata": {},
   "source": [
    "## Avoiding Context Window Problems\n",
    "\n",
    "The above example shows the main uses of the Metaphor tool. We can easily retrieve a clean list of links related to a query, and then we can fetch the content of the article as a cleaned up html extract. \n",
    "\n",
    "We can see that the content of the articles is somewhat long compared to current LLM context windows, and so to allow retrieval and summary of many documents we will set up and use another tool from LlamaIndex that allows us to load text into a VectorStore, and query it for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a017cc61-1696-4a03-8d09-a628f9049cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tool_spec.load_and_search.base import LoadAndSearchToolSpec\n",
    "\n",
    "# The retrieve documents tool is the second in the tool list, as seen above\n",
    "wrapped_retrieve = LoadAndSearchToolSpec.from_defaults(\n",
    "    metaphor_tool_list[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b47437-8f6d-4e94-97ca-4e35f78336f2",
   "metadata": {},
   "source": [
    "Our wrapped retrieval tools seperate loading and reading into seperate interfaces. We use `load` to load the documents into the vector store, and `read` to query the vector store. Let's try it out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f81bd3-a5b9-452c-93f4-91d16c4c0df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA Transformer is a class of sequence processing models that use attention and feedforward operations to achieve state-of-the-art and efficient sequence processing without recurrent units or convolution.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_retrieve.load(['iEYMai5rS9k0hN5_BH0VZg'])\n",
    "wrapped_retrieve.read('what is a transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be6977-c4e8-43a4-99be-3322d4b72b07",
   "metadata": {},
   "source": [
    "## Creating the Agent\n",
    "\n",
    "We now have a total of 5 tools that we will make available to the Agent, allowing it to use Metaphor's services to it's full potential:\n",
    "\n",
    "`metaphor_search`: To search for a list of relevant articles\n",
    "\n",
    "`retrieve_documents`: To load the articles into a vector store\n",
    "\n",
    "`read_retrieve_documents`: To query the articles loaded into the vector store\n",
    "\n",
    "`find_similar`: To find articles similiar to a given URL\n",
    "\n",
    "`get_date`: A simple utility method to get todays date for the Agent, for better usage of date based filtering\n",
    "\n",
    "\n",
    "We will pass all of these functions into our Agent, and test it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a893f26-dbb6-4b72-9795-702eaf749564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We don't give the Agent our unwrapped retrieve document tools, instead passing the wrapped tools\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [metaphor_tool_list[0], *wrapped_retrieve.to_tool_list(), metaphor_tool_list[2], metaphor_tool_list[3]],\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5835d058-da9c-4d42-9d2a-941c73b88a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: current_date with args: {}\n",
      "Got output: 2023-08-16\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: metaphor_search with args: {\n",
      "  \"query\": \"news about superconductors\",\n",
      "  \"num_results\": 5,\n",
      "  \"start_published_date\": \"2023-07-16\",\n",
      "  \"end_published_date\": \"2023-08-16\"\n",
      "}\n",
      "Got output: [{'title': 'Why a \"room-temperature superconductor\" would be a huge deal', 'url': 'https://www.vox.com/future-perfect/23816753/superconductor-room-temperature-lk99-quantum-fusion', 'id': 'cD13waRIIwL6_S4SlIRHjQ'}, {'title': 'Korean team claims to have created the first room-temperature, ambient-pressure superconductor', 'url': 'https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html', 'id': 'jXsm1TpKVy9zg5kH3bRuag'}, {'title': \"Room-temperature superconductors: Here's everything you need to know\", 'url': 'https://www.newscientist.com/article/2385270-room-temperature-superconductors-heres-everything-you-need-to-know/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home', 'id': 'WD9B1aQrsQFqUl7k6Yk2EQ'}, {'title': 'Get ready for the age of the superconductor (maybe)', 'url': 'https://newatlas.com/science/get-ready-for-the-age-of-the-superconductor-maybe/', 'id': '1-PPJlmzmhdzV8cjvGftLw'}, {'title': \"Room-temperature superconductor 'breakthrough' met with scepticism\", 'url': 'https://www.newscientist.com/article/2384782-room-temperature-superconductor-breakthrough-met-with-scepticism/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home', 'id': 'sl8QkKlqm1IWASBsfDcg2A'}]\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: retrieve_documents with args: {\n",
      "  \"ids\": [\"cD13waRIIwL6_S4SlIRHjQ\", \"jXsm1TpKVy9zg5kH3bRuag\", \"WD9B1aQrsQFqUl7k6Yk2EQ\", \"1-PPJlmzmhdzV8cjvGftLw\", \"sl8QkKlqm1IWASBsfDcg2A\"]\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_retrieve_documents\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: read_retrieve_documents with args: {\n",
      "  \"query\": \"summarize the news published in the last month on superconductors\"\n",
      "}\n",
      "Got output: \n",
      "In the last month, news has been circulating about a new superconducting material called LK-99, which is purported to retain its properties at room temperature and ambient atmospheric pressure. This announcement has been met with skepticism, as no one has been able to verifiably replicate the research. If true, this material could revolutionize power generation and transmission, energy storage, fusion power, and computing. It could also enable floating buildings and other transportation applications.\n",
      "========================\n",
      "In the last month, there have been several news articles published about superconductors. Here are some key highlights:\n",
      "\n",
      "1. \"Why a 'room-temperature superconductor' would be a huge deal\" - This article from Vox discusses the potential impact of a room-temperature superconductor called LK-99. If such a material exists, it could revolutionize various industries, including power generation, energy storage, and computing. [Read more](https://www.vox.com/future-perfect/23816753/superconductor-room-temperature-lk99-quantum-fusion)\n",
      "\n",
      "2. \"Korean team claims to have created the first room-temperature, ambient-pressure superconductor\" - According to Phys.org, a Korean team has claimed to have developed the first room-temperature, ambient-pressure superconductor. However, the research has faced skepticism, and further verification is needed. [Read more](https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html)\n",
      "\n",
      "3. \"Room-temperature superconductors: Here's everything you need to know\" - New Scientist provides an overview of room-temperature superconductors, discussing their potential applications and the challenges in achieving them. [Read more](https://www.newscientist.com/article/2385270-room-temperature-superconductors-heres-everything-you-need-to-know/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home)\n",
      "\n",
      "4. \"Get ready for the age of the superconductor (maybe)\" - This article from New Atlas explores the possibilities and potential advancements that could come with the development of superconductors. It highlights the potential impact on various industries and technologies. [Read more](https://newatlas.com/science/get-ready-for-the-age-of-the-superconductor-maybe/)\n",
      "\n",
      "5. \"Room-temperature superconductor 'breakthrough' met with skepticism\" - New Scientist reports on the skepticism surrounding the recent claims of a room-temperature superconductor breakthrough. The article discusses the need for further evidence and replication of the research. [Read more](https://www.newscientist.com/article/2384782-room-temperature-superconductor-breakthrough-met-with-scepticism/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=home)\n",
      "\n",
      "These articles highlight the excitement and skepticism surrounding the possibility of room-temperature superconductors and their potential impact on various industries.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat('Can you summarize the news published in the last month on superconductors'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee91ca-6730-4fdd-8189-ac21022f34f1",
   "metadata": {},
   "source": [
    "We asked the agent to retrieve documents related to superconductors from this month. It used the `get_date` tool to determine the current month, and then applied the filters in Metaphor based on publication date when calling `metaphor_search`. It then loaded the documents using `retrieve_documents` and read them using `read_retrieve_documents`.\n",
    "\n",
    "We can make another query to the vector store to read from it again, now that the articles are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d279f9f-c305-43ed-ae54-334c0ec6f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: find_similar with args: {\n",
      "  \"url\": \"https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html\",\n",
      "  \"num_results\": 1\n",
      "}\n",
      "Got output: [{'title': 'Korean team claims to have created the first room-temperature, ambient-pressure superconductor', 'url': 'https://phys.org/news/2023-07-korean-team-room-temperature-ambient-pressure-superconductor.html', 'id': 'jXsm1TpKVy9zg5kH3bRuag'}]\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: retrieve_documents with args: {\n",
      "  \"ids\": [\"jXsm1TpKVy9zg5kH3bRuag\"]\n",
      "}\n",
      "Got output: Content loaded! You can now search the information using read_retrieve_documents\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: read_retrieve_documents with args: {\n",
      "  \"query\": \"process to make the new superconductor\"\n",
      "}\n",
      "Got output: \n",
      "The process to make the new superconductor, LK-99, involves mixing powdered compounds containing lead, oxygen, sulphur and phosphorus, then heating them at a high temperature for several hours. This makes the powders chemically react and transform into a dark grey solid.\n",
      "========================\n",
      "The process to make the new superconductor, LK-99, involves mixing powdered compounds containing lead, oxygen, sulfur, and phosphorus. These powders are then heated at a high temperature for several hours. The heating process allows the powders to chemically react and transform into a dark grey solid. This solid material exhibits superconducting properties at room temperature and ambient pressure.\n"
     ]
    }
   ],
   "source": [
    "print(agent.chat('whats the process to make the new superconductor?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
